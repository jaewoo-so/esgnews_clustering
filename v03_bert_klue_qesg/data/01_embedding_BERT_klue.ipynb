{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddd4fb6-13a3-48da-8188-4fc1b3cffd57",
   "metadata": {},
   "source": [
    "Autor: so jae woo    \n",
    "Data src :  qesg news / 28 cat / 본문 포함.    \n",
    "Summary : KLUE기반의 버트 사정 임베딩 모델 사용.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588d4543-b74a-430c-b1d6-b30d62ebb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import transformers\n",
    "transformers.__version__ # \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import TextClassificationPipeline\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from lib_offline import prep_mecab_noun_offline\n",
    "from lib2 import text_preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500f9a2-4bc6-4e44-bbd5-2f80eb8371c9",
   "metadata": {},
   "source": [
    "# 데이터 준비 \n",
    "## QESG 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04865168-2422-4016-9935-a7617f43612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qesg = pd.read_parquet(\"./df_qesg28_final.pq\")\n",
    "df_qesg = df_qesg.loc[df_qesg.y.notna()]\n",
    "df_qesg = df_qesg.loc[df_qesg.y != '기타']\n",
    "df_qesg = df_qesg.loc[df_qesg.y != '제외']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826e2161-3563-4570-a0a9-a9f296a2d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(df_qesg.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df1ada-af2c-487d-9af3-a192b07cab76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 데이터 클리닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c08d8-3cfc-4ea1-9cc3-10ce6c58e001",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 짧은 본문 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380ebf-ddec-4fac-859d-4e0bee2dcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contents_clean(row):\n",
    "    c = row.contents\n",
    "    cc = row.contents_clean\n",
    "    \n",
    "    if len(c) == 0:\n",
    "        return False\n",
    "    \n",
    "    if len(cc)/len(c)> 0.7:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cf2fc7-cc2e-4832-8487-d92db97033fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bol_list = [ check_contents_clean(row) for i , row in df_qesg.iterrows() ]\n",
    "df_qesg['size'] = df_qesg[bol_list].contents_clean.apply(lambda x : len(x))\n",
    "df_qesg = df_qesg.loc[df_qesg['size'] > 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c610c09-c74c-4b72-94b6-b2107e0763b1",
   "metadata": {},
   "source": [
    "# BERT 임베딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082201fe-a739-435b-86ca-6c78e3c078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7f30fb-ae9f-48e4-9b78-f6a842598f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer \n",
    "max_seq_len = 2028\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base', truncation=True, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e518bd76-f57e-41ea-a8ce-fee4096e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라벨 숫자 인코딩\n",
    "lbl_name = sorted(df_qesg['y'].unique().tolist())\n",
    "lbl_num = list(range(len(lbl_name)))\n",
    "lbl_name2num = dict(zip(lbl_name,lbl_num))\n",
    "lbl_num2name = dict(zip(lbl_num,lbl_name))\n",
    "df_qesg['y2num']  = df_qesg.y.apply(lambda x : lbl_name2num[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ac2f3-71c8-429f-a203-170fc6a3955e",
   "metadata": {},
   "source": [
    "# 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7b2bbe-537f-4600-b5bb-1d40f45dc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_qesg['contents_clean'].tolist())\n",
    "y = np.array(df_qesg['y2num'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77417610-ef5f-4986-b90a-5a33dd919687",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a02fb75-11f2-49eb-ac82-1d6b8dd9e6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342f0edb-afc9-4658-b5f4-bf1a79e8fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 원큐\n",
    "class_weights = class_weight.compute_class_weight(class_weight  = 'balanced',classes= np.unique(y), y = y) \n",
    "class_weights_dict = dict(zip(  list(range(len(class_weights))),class_weights))\n",
    "\n",
    "X_train = tokenizer(X.tolist(), truncation=True, padding=True)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_train),\n",
    "    y\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff9dcf7-f0ac-4bf8-88ef-9f306e922716",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_save = tf.keras.callbacks.ModelCheckpoint(\n",
    "    './model',\n",
    "    monitor = 'accuracy',\n",
    "  \n",
    "    save_best_only = True,\n",
    "    save_weights_only = False,\n",
    "    mode= 'auto',\n",
    "    save_freq='epoch',\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21970b-46e5-4f17-82d2-9ccecf037f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=df_qesg['y2num'].nunique(), from_pt=True)\n",
    "loss =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "history =model.fit(\n",
    "train_dataset.shuffle(1000).batch(16), epochs=10, batch_size=32,class_weight= class_weights_dict,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb296dd-5162-49ab-9d69-67364c07d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weight_28_embedding', save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8089e-e718-4bfb-a958-2013279cb057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a019ba70-cb52-46ce-a983-96b745b227a5",
   "metadata": {},
   "source": [
    "## 임베딩 결과 저장하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7614b6b-7c81-4c1b-b1a8-b05c301483b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train = tokenizer(X.tolist(), truncation=True, padding=True)\n",
    "del tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=df_qesg['y2num'].nunique(), from_pt=True)\n",
    "model.load_weights('model_weight_28_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3fe7e1f-3b52-47e4-8247-cb6811b96e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1e0ae3c1be0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=df_qesg['y2num'].nunique(), from_pt=True)\n",
    "model.load_weights('model_weight_28_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41cafb-f188-4099-ae96-9b4806f15210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res_emb = model.bert.embeddings(X_train['input_ids'],X_train['attention_mask'],X_train['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3195500e-2e8f-412a-8197-12cfb76bece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c855cb93-61af-4cab-b31c-33aab086f9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a6f261-fe75-4111-b273-8d979b29f3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp38]",
   "language": "python",
   "name": "conda-env-nlp38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
