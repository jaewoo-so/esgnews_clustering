{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddd4fb6-13a3-48da-8188-4fc1b3cffd57",
   "metadata": {},
   "source": [
    "Autor: so jae woo    \n",
    "Data src :  qesg news / 28 cat / 본문 포함.    \n",
    "Summary : KLUE기반의 버트 사정 임베딩 모델 사용.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "588d4543-b74a-430c-b1d6-b30d62ebb730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import transformers\n",
    "transformers.__version__ # \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import TextClassificationPipeline\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from lib_offline import prep_mecab_noun_offline\n",
    "from lib2 import text_preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500f9a2-4bc6-4e44-bbd5-2f80eb8371c9",
   "metadata": {},
   "source": [
    "# 데이터 준비 \n",
    "## QESG 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f32bc1c-1753-4a22-8fdf-94b962cc6241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qesg = pickle.load(open(\"./data_all_na_news.pkl\",'rb'))\n",
    "df_qesg = df_qesg.dropna( subset = ['contents','label_category'] , axis = 0 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df1ada-af2c-487d-9af3-a192b07cab76",
   "metadata": {},
   "source": [
    "### 데이터 클리닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989e1d57-320b-404c-bf93-b6ab150ff006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 32.1 s\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_qesg['contents_clean'] = df_qesg['contents'].apply(lambda x : text_preprocessing.manual_remove(text_preprocessing.cleaning_korean(x)))\n",
    "df_qesg = df_qesg[['id', 'keyword', 'title', 'link', 'contents', 'label_category','label_sentiment', 'contents_clean']]\n",
    "\n",
    "#컬럼명 바꾸기\n",
    "df_qesg.columns = ['id', 'keyword', 'title', 'link', 'contents', 'y','label_sentiment', 'contents_clean']\n",
    "df_qesg = df_qesg.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c08d8-3cfc-4ea1-9cc3-10ce6c58e001",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 짧은 본문 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc380ebf-ddec-4fac-859d-4e0bee2dcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contents_clean(row):\n",
    "    c = row.contents\n",
    "    cc = row.contents_clean\n",
    "    \n",
    "    if len(c) == 0:\n",
    "        return False\n",
    "    \n",
    "    if len(cc)/len(c)> 0.7:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69cf2fc7-cc2e-4832-8487-d92db97033fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qesg = df_qesg.loc[df_qesg.y != '제외']\n",
    "bol_list = [ check_contents_clean(row) for i , row in df_qesg.iterrows() ]\n",
    "df_qesg['size'] = df_qesg[bol_list].contents_clean.apply(lambda x : len(x))\n",
    "df_qesg = df_qesg.loc[df_qesg['size'] > 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c610c09-c74c-4b72-94b6-b2107e0763b1",
   "metadata": {},
   "source": [
    "# BERT 임베딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "082201fe-a739-435b-86ca-6c78e3c078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d7f30fb-ae9f-48e4-9b78-f6a842598f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer \n",
    "max_seq_len = 2028\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base', truncation=True, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e518bd76-f57e-41ea-a8ce-fee4096e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라벨 숫자 인코딩\n",
    "lbl_name = sorted(df_qesg['y'].unique().tolist())\n",
    "lbl_num = list(range(len(lbl_name)))\n",
    "lbl_name2num = dict(zip(lbl_name,lbl_num))\n",
    "lbl_num2name = dict(zip(lbl_num,lbl_name))\n",
    "df_qesg['y2num']  = df_qesg.y.apply(lambda x : lbl_name2num[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ac2f3-71c8-429f-a203-170fc6a3955e",
   "metadata": {},
   "source": [
    "# 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c7b2bbe-537f-4600-b5bb-1d40f45dc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_qesg['contents_clean'].tolist())\n",
    "y = np.array(df_qesg['y2num'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "342f0edb-afc9-4658-b5f4-bf1a79e8fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원큐\n",
    "class_weights = class_weight.compute_class_weight(class_weight  = 'balanced',classes= np.unique(y), y = y) \n",
    "class_weights_dict = dict(zip(  list(range(len(class_weights))),class_weights))\n",
    "\n",
    "X_train = tokenizer(X.tolist(), truncation=True, padding=True)\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(X_train),\n",
    "    y\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de21970b-46e5-4f17-82d2-9ccecf037f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "2663/2663 [==============================] - 1135s 422ms/step - loss: 1.9848 - accuracy: 0.5143\n",
      "Epoch 2/7\n",
      "2663/2663 [==============================] - 1113s 418ms/step - loss: 1.4196 - accuracy: 0.6461\n",
      "Epoch 3/7\n",
      "2663/2663 [==============================] - 1111s 417ms/step - loss: 1.1418 - accuracy: 0.6951\n",
      "Epoch 4/7\n",
      "2663/2663 [==============================] - 1112s 417ms/step - loss: 1.1469 - accuracy: 0.6581\n",
      "Epoch 5/7\n",
      "2663/2663 [==============================] - 1121s 421ms/step - loss: 0.8458 - accuracy: 0.7738\n",
      "Epoch 6/7\n",
      "2663/2663 [==============================] - 1119s 420ms/step - loss: 0.8794 - accuracy: 0.7627\n",
      "Epoch 7/7\n",
      "2663/2663 [==============================] - 1115s 419ms/step - loss: 1.3036 - accuracy: 0.6285\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=df_qesg['y2num'].nunique(), from_pt=True)\n",
    "loss =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "history =model.fit(\n",
    "train_dataset.shuffle(1000).batch(16), epochs=7, batch_size=32,class_weight= class_weights_dict,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23f245-4fb0-4c47-b595-09652895112c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp38]",
   "language": "python",
   "name": "conda-env-nlp38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
