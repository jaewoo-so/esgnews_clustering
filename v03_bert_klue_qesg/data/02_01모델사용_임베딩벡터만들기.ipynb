{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddd4fb6-13a3-48da-8188-4fc1b3cffd57",
   "metadata": {},
   "source": [
    "Autor: so jae woo    \n",
    "Data src :  qesg news / 28 cat / 본문 포함.    \n",
    "Summary : KLUE기반의 버트 사정 임베딩 모델 사용.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "588d4543-b74a-430c-b1d6-b30d62ebb730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import transformers\n",
    "transformers.__version__ # \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import TextClassificationPipeline\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from lib_offline import prep_mecab_noun_offline\n",
    "from lib2 import text_preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500f9a2-4bc6-4e44-bbd5-2f80eb8371c9",
   "metadata": {},
   "source": [
    "# 데이터 준비 \n",
    "## QESG 데이터 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04865168-2422-4016-9935-a7617f43612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qesg = pd.read_parquet(\"./df_qesg28_final.pq\")\n",
    "df_qesg = df_qesg.loc[df_qesg.y.notna()]\n",
    "df_qesg = df_qesg.loc[df_qesg.y != '기타']\n",
    "df_qesg = df_qesg.loc[df_qesg.y != '제외']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826e2161-3563-4570-a0a9-a9f296a2d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.unique(df_qesg.y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df1ada-af2c-487d-9af3-a192b07cab76",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 데이터 클리닝"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c08d8-3cfc-4ea1-9cc3-10ce6c58e001",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 짧은 본문 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc380ebf-ddec-4fac-859d-4e0bee2dcf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contents_clean(row):\n",
    "    c = row.contents\n",
    "    cc = row.contents_clean\n",
    "    \n",
    "    if len(c) == 0:\n",
    "        return False\n",
    "    \n",
    "    if len(cc)/len(c)> 0.7:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69cf2fc7-cc2e-4832-8487-d92db97033fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bol_list = [ check_contents_clean(row) for i , row in df_qesg.iterrows() ]\n",
    "df_qesg['size'] = df_qesg[bol_list].contents_clean.apply(lambda x : len(x))\n",
    "df_qesg = df_qesg.loc[df_qesg['size'] > 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c610c09-c74c-4b72-94b6-b2107e0763b1",
   "metadata": {},
   "source": [
    "# BERT 임베딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "082201fe-a739-435b-86ca-6c78e3c078ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, labels, max_seq_len, tokenizer):\n",
    "    \n",
    "    input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []\n",
    "    \n",
    "    for example, label in tqdm(zip(examples, labels), total=len(examples)):\n",
    "        input_id = tokenizer.encode(example, max_length=max_seq_len, pad_to_max_length=True)\n",
    "        padding_count = input_id.count(tokenizer.pad_token_id)\n",
    "        attention_mask = [1] * (max_seq_len - padding_count) + [0] * padding_count\n",
    "        token_type_id = [0] * max_seq_len\n",
    "\n",
    "        assert len(input_id) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_id), max_seq_len)\n",
    "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
    "        assert len(token_type_id) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_id), max_seq_len)\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        data_labels.append(label)\n",
    "\n",
    "    input_ids = np.array(input_ids, dtype=int)\n",
    "    attention_masks = np.array(attention_masks, dtype=int)\n",
    "    token_type_ids = np.array(token_type_ids, dtype=int)\n",
    "\n",
    "    data_labels = np.asarray(data_labels, dtype=np.int32)\n",
    "\n",
    "    return (input_ids, attention_masks, token_type_ids), data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e518bd76-f57e-41ea-a8ce-fee4096e2c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 라벨 숫자 인코딩\n",
    "lbl_name = sorted(df_qesg['y'].unique().tolist())\n",
    "lbl_num = list(range(len(lbl_name)))\n",
    "lbl_name2num = dict(zip(lbl_name,lbl_num))\n",
    "lbl_num2name = dict(zip(lbl_num,lbl_name))\n",
    "df_qesg['y2num']  = df_qesg.y.apply(lambda x : lbl_name2num[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97ac2f3-71c8-429f-a203-170fc6a3955e",
   "metadata": {},
   "source": [
    "# 임베딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7f30fb-ae9f-48e4-9b78-f6a842598f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tokenizer \n",
    "max_seq_len = 2028\n",
    "tokenizer = BertTokenizer.from_pretrained('klue/bert-base', truncation=True, max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c7b2bbe-537f-4600-b5bb-1d40f45dc57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df_qesg['contents_clean'].tolist())\n",
    "y = np.array(df_qesg['y2num'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7614b6b-7c81-4c1b-b1a8-b05c301483b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 40s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x258af4c8f70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "x_src = X.tolist()\n",
    "X_train = tokenizer(x_src, truncation=True, padding=True)\n",
    "del tokenizer\n",
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=df_qesg['y2num'].nunique(), from_pt=True)\n",
    "model.load_weights('model_weight_28_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41cafb-f188-4099-ae96-9b4806f15210",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res_emb = model.bert.embeddings(X_train['input_ids'],X_train['attention_mask'],X_train['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c855cb93-61af-4cab-b31c-33aab086f9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp38]",
   "language": "python",
   "name": "conda-env-nlp38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
