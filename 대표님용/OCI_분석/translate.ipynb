{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091d9cba-98e5-487e-aea3-86c92ff61baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import lib\n",
    "from textwrap import dedent\n",
    "from googletrans import Translator\n",
    "from tqdm.auto import tqdm\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77b6ed8-b903-482e-875b-0ba8900e0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./scrape_oci.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dd7077-91f7-4217-8fdc-778cca3aff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_korean(content):\n",
    "    content = content.replace('\\n\\n','')\n",
    "    content = ' '.join(content.split(' '))\n",
    "\n",
    "    #í´ë¦¬ë‹1\n",
    "    content = re.sub(r'... ê¸°ì', '', content)\n",
    "    content = re.sub(r'...ê¸°ì', '', content)\n",
    "\n",
    "    #ê´„í˜¸ì•ˆ ì œê±° \n",
    "    ''' ê´„í˜¸ì•ˆ ë‚´ìš© ì œê±°í•˜ê¸° '''\n",
    "    pattern_bracket1 = r'\\([^)]*\\)'\n",
    "    pattern_bracket2 = r'\\{[^)]*\\}'\n",
    "    pattern_bracket3 = r'\\<[^)]*\\>'\n",
    "    pattern_bracket4 = r'\\[[^)]*\\]'\n",
    "\n",
    "    x = 'ì´ê±´ {ê´„í˜¸ ì•ˆì˜ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼} ì‚­ì œí•˜ëŠ” ì½”ë“œë‹¤.'\n",
    "    content = re.sub(pattern=pattern_bracket1, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    content = re.sub(pattern=pattern_bracket2, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    content = re.sub(pattern=pattern_bracket3, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    content = re.sub(pattern=pattern_bracket4, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    #í´ë¦¬ë‹\n",
    "    content = content.lower() #lower case\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mailì œê±°\n",
    "    content = re.sub(pattern=pattern, repl='', string=content)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URLì œê±°\n",
    "    content = re.sub(pattern=pattern, repl='', string=content)\n",
    "    # íŠ¹ìˆ˜ê¸°í˜¸ íŠ¹ìˆ˜\n",
    "    content = re.sub('[-=+#/\\?:^$@*\\\"â€»~%ã†!ã€\\\\â€˜|\\(\\)\\[\\]\\<\\>`\\'â€¦ã€‹Î”â–³Â·â€œ=â—<>â–·]', '', content)\n",
    "    content = re.sub('[ä¸€-é¾¥]', '',content)\n",
    "    #pattern = '[^\\w\\s]'         # íŠ¹ìˆ˜ê¸°í˜¸ì œê±°\n",
    "    #content = re.sub(r'[@%\\\\*=()/~#&\\+Ã¡?\\xc3\\xa1\\-\\|\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(content)) #remove punctuation\n",
    "    content = re.sub(r\"[^a-zA-Z0-9ê°€-í£\\.\\s]\",\"\",content)\n",
    "    content = re.sub(pattern=pattern, repl='', string=content)\n",
    "    content = re.sub(r'\\s+', ' ', content) #remove extra space\n",
    "    content = re.sub(r'<[^>]+>','',content) #remove Html tags\n",
    "    content = re.sub(r'\\s+', ' ', content) #remove spaces\n",
    "    content = re.sub(r\"^\\s+\", '', content) #remove space from start\n",
    "    content = re.sub(r'\\s+$', '', content) #remove space from the end\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a18716b-d8d3-426f-a95e-1b985ad68b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contents = []\n",
    "for i, row in df.iterrows():\n",
    "    tit = row.Titles\n",
    "    cot = row.Contents\n",
    "    cot_all = str(tit) + '.' + str(cot)\n",
    "    all_contents.append(cleaning_korean(cot_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69770b8-e0c8-42bb-8796-e0e924b9d878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd0e9d1d1c9469fbf216a3d40e1a1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 50.2 s\n",
      "Wall time: 2h 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "contents_en = []\n",
    "for x in tqdm(all_contents):\n",
    "    try:\n",
    "        src = \"ë‹¤ìŒì˜ ë¬¸ì¥ì´ ê¸ì •,ì¤‘ë¦½,ë¶€ì • ì¤‘ ì–´ëŠ ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ ì•Œë ¤ì¤˜ \" + '\"' + str(x) + '\"'\n",
    "        src_en = translator.translate(src).text\n",
    "        contents_en.append(src_en)\n",
    "    except:\n",
    "        contents_en.append('ì¤‘ë¦½')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c95381-f083-44ec-a979-8875b9c6a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contents_clean'] = all_contents\n",
    "df['contents_en'] = contents_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a24b72df-d159-481b-b53c-ca9ab73db394",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = []\n",
    "for en in contents_en: \n",
    "    res = re.findall(r'\\\".*?\\\"', en)\n",
    "    if res == []:\n",
    "        qq.append('')\n",
    "    else:\n",
    "        qq.append(res[0].replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "552a6dd0-122f-47cc-9550-f1c3e2e0c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qa'] = qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32f0055c-bacd-4500-8150-7c0f97dfd662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Contents</th>\n",
       "      <th>contents_clean</th>\n",
       "      <th>contents_en</th>\n",
       "      <th>qa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.02.23 10:52</td>\n",
       "      <td>ì¤‘êµ­ ì‹ ì¥ì§€êµ¬ì— ì§€ì§„ ì§„ë„7.3</td>\n",
       "      <td>ëƒ‰ë¬´</td>\n",
       "      <td>ì¤‘êµ­ ì‹ ì¥ì§€êµ¬ì— ì§€ì§„ ì§„ë„7.3.ëƒ‰ë¬´</td>\n",
       "      <td>Please tell me which categories of the followi...</td>\n",
       "      <td>Earthquake in China's Xinjiang District 7.3. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023.02.23 10:51</td>\n",
       "      <td>ì‹ í•œ [2]</td>\n",
       "      <td>í˜¼ì ëˆŒëŸ¬ëŒ€ëŠë¼ ì• ì“´ë‹¤\\nê³µë§¤ìƒí™˜ í•˜ë ¤ë©´ ëª»ì˜¤ë¥´ê²Œ ëˆŒëŸ¬ì•¼ì§€\\nê·¸ë ‡ì§€ë§Œ ê·¸ê²ƒë„ ê³§í•œê³„...</td>\n",
       "      <td>ì‹ í•œ .í˜¼ì ëˆŒëŸ¬ëŒ€ëŠë¼ ì• ì“´ë‹¤ ê³µë§¤ìƒí™˜ í•˜ë ¤ë©´ ëª»ì˜¤ë¥´ê²Œ ëˆŒëŸ¬ì•¼ì§€ ê·¸ë ‡ì§€ë§Œ ê·¸ê²ƒë„ ê³§...</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023.02.23 10:50</td>\n",
       "      <td>ì• ì“´ë‹¤ ë¹„ì‹¸ê²Œ</td>\n",
       "      <td>íŒ”ë ¤ê³  ì € ë§¤ìˆ˜í˜¸ê°€ ë´ë¼ ã…‹ã…‹</td>\n",
       "      <td>ì• ì“´ë‹¤ ë¹„ì‹¸ê²Œ.íŒ”ë ¤ê³  ì € ë§¤ìˆ˜í˜¸ê°€ ë´ë¼</td>\n",
       "      <td>Tell me which categories of the following sent...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023.02.23 10:42</td>\n",
       "      <td>ìœ ì•ˆíƒ€ ã…‰ã…‰</td>\n",
       "      <td>ì´ íƒ€ì´ë°ì— ë§¤ë„ë¥¼ í•˜ë‹ˆê¹Œ ë©”ì´ì € ì¦ê¶Œì‚¬ëŠ” ë‹¤ ëˆë²Œë•Œ ì ìì „í™˜í•˜ì§€ ã…‹ã…‹ã…‹</td>\n",
       "      <td>ìœ ì•ˆíƒ€ .ì´ íƒ€ì´ë°ì— ë§¤ë„ë¥¼ í•˜ë‹ˆê¹Œ ë©”ì´ì € ì¦ê¶Œì‚¬ëŠ” ë‹¤ ëˆë²Œë•Œ ì ìì „í™˜í•˜ì§€</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023.02.23 10:33</td>\n",
       "      <td>í”„ë¡œê·¸ë¨ ê°œ ê°„ë³´ë„¤ [1]</td>\n",
       "      <td>ë†’ê²Œ íŒ”ë ¤ê³  ã…‹ã…‹ã…‹</td>\n",
       "      <td>í”„ë¡œê·¸ë¨ ê°œ ê°„ë³´ë„¤ .ë†’ê²Œ íŒ”ë ¤ê³ </td>\n",
       "      <td>Tell me which categories of the following sent...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>2022.10.24 09:07</td>\n",
       "      <td>ë¯¸êµ­í™•ì¥.. [2]</td>\n",
       "      <td>êµ¿..ë°”ë¡œì´ê±°ì§€.\\nëª¨ë ˆì‹¤ì ë„ êµ¿.\\në‹¬ë ¤ë³´ì</td>\n",
       "      <td>ë¯¸êµ­í™•ì¥.. .êµ¿..ë°”ë¡œì´ê±°ì§€. ëª¨ë ˆì‹¤ì ë„ êµ¿. ë‹¬ë ¤ë³´ì</td>\n",
       "      <td>Please tell me which categories of the followi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>2022.10.23 23:10</td>\n",
       "      <td>ë‚´ì¼ [2]</td>\n",
       "      <td>ì¥ ê´œì°®ì„ê¹Œìš”</td>\n",
       "      <td>ë‚´ì¼ .ì¥ ê´œì°®ì„ê¹Œìš”</td>\n",
       "      <td>Tell me which categories of the following sent...</td>\n",
       "      <td>Tomorrow. Is it okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>2022.10.23 18:51</td>\n",
       "      <td>ì´ìš°í˜„</td>\n",
       "      <td>3ì¼ë‚¨ì•˜ë‹¤</td>\n",
       "      <td>ì´ìš°í˜„.3ì¼ë‚¨ì•˜ë‹¤</td>\n",
       "      <td>Please tell me which category of the following...</td>\n",
       "      <td>Lee Woo -hyun. 3 There is a day left.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>2022.10.23 15:03</td>\n",
       "      <td>ê¹”ê¹” ë§ˆë…€ ë“±ì¥!!!!!!!! [6]</td>\n",
       "      <td>ì–´ì´êµ¬ í˜¸9ëœì•„ ì•„ì§ë„ ì–¸ë‹ˆë§ì•ˆë“£ê³  í˜„ì‹¤ë¶€ì •í•˜ëŠ” ëª»ë‚œì´ë“¤ ì—†ã…ˆ ã…”~? ã…‹ ã…”~</td>\n",
       "      <td>ê¹”ê¹” ë§ˆë…€ ë“±ì¥ .ì–´ì´êµ¬ í˜¸9ëœì•„ ì•„ì§ë„ ì–¸ë‹ˆë§ì•ˆë“£ê³  í˜„ì‹¤ë¶€ì •í•˜ëŠ” ëª»ë‚œì´ë“¤ ì—†</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668</th>\n",
       "      <td>2022.10.23 12:43</td>\n",
       "      <td>ì™¸ì¸ê¸°ê°„ë“¤ [1]</td>\n",
       "      <td>ê¸‰ë“±í•¨ì„œ ìŒëŒì´ê°€\\nëŒ€ì„¸ìƒìŠ¹ ëŒì…ì˜ˆìƒ</td>\n",
       "      <td>ì™¸ì¸ê¸°ê°„ë“¤ .ê¸‰ë“±í•¨ì„œ ìŒëŒì´ê°€ ëŒ€ì„¸ìƒìŠ¹ ëŒì…ì˜ˆìƒ</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6669 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dates                Titles  \\\n",
       "0     2023.02.23 10:52     ì¤‘êµ­ ì‹ ì¥ì§€êµ¬ì— ì§€ì§„ ì§„ë„7.3   \n",
       "1     2023.02.23 10:51                ì‹ í•œ [2]   \n",
       "2     2023.02.23 10:50               ì• ì“´ë‹¤ ë¹„ì‹¸ê²Œ   \n",
       "3     2023.02.23 10:42                ìœ ì•ˆíƒ€ ã…‰ã…‰   \n",
       "4     2023.02.23 10:33        í”„ë¡œê·¸ë¨ ê°œ ê°„ë³´ë„¤ [1]   \n",
       "...                ...                   ...   \n",
       "6664  2022.10.24 09:07            ë¯¸êµ­í™•ì¥.. [2]   \n",
       "6665  2022.10.23 23:10                ë‚´ì¼ [2]   \n",
       "6666  2022.10.23 18:51                   ì´ìš°í˜„   \n",
       "6667  2022.10.23 15:03  ê¹”ê¹” ë§ˆë…€ ë“±ì¥!!!!!!!! [6]   \n",
       "6668  2022.10.23 12:43             ì™¸ì¸ê¸°ê°„ë“¤ [1]   \n",
       "\n",
       "                                               Contents  \\\n",
       "0                                                    ëƒ‰ë¬´   \n",
       "1     í˜¼ì ëˆŒëŸ¬ëŒ€ëŠë¼ ì• ì“´ë‹¤\\nê³µë§¤ìƒí™˜ í•˜ë ¤ë©´ ëª»ì˜¤ë¥´ê²Œ ëˆŒëŸ¬ì•¼ì§€\\nê·¸ë ‡ì§€ë§Œ ê·¸ê²ƒë„ ê³§í•œê³„...   \n",
       "2                                      íŒ”ë ¤ê³  ì € ë§¤ìˆ˜í˜¸ê°€ ë´ë¼ ã…‹ã…‹   \n",
       "3              ì´ íƒ€ì´ë°ì— ë§¤ë„ë¥¼ í•˜ë‹ˆê¹Œ ë©”ì´ì € ì¦ê¶Œì‚¬ëŠ” ë‹¤ ëˆë²Œë•Œ ì ìì „í™˜í•˜ì§€ ã…‹ã…‹ã…‹   \n",
       "4                                            ë†’ê²Œ íŒ”ë ¤ê³  ã…‹ã…‹ã…‹   \n",
       "...                                                 ...   \n",
       "6664                          êµ¿..ë°”ë¡œì´ê±°ì§€.\\nëª¨ë ˆì‹¤ì ë„ êµ¿.\\në‹¬ë ¤ë³´ì   \n",
       "6665                                            ì¥ ê´œì°®ì„ê¹Œìš”   \n",
       "6666                                              3ì¼ë‚¨ì•˜ë‹¤   \n",
       "6667        ì–´ì´êµ¬ í˜¸9ëœì•„ ì•„ì§ë„ ì–¸ë‹ˆë§ì•ˆë“£ê³  í˜„ì‹¤ë¶€ì •í•˜ëŠ” ëª»ë‚œì´ë“¤ ì—†ã…ˆ ã…”~? ã…‹ ã…”~   \n",
       "6668                               ê¸‰ë“±í•¨ì„œ ìŒëŒì´ê°€\\nëŒ€ì„¸ìƒìŠ¹ ëŒì…ì˜ˆìƒ   \n",
       "\n",
       "                                         contents_clean  \\\n",
       "0                                  ì¤‘êµ­ ì‹ ì¥ì§€êµ¬ì— ì§€ì§„ ì§„ë„7.3.ëƒ‰ë¬´   \n",
       "1     ì‹ í•œ .í˜¼ì ëˆŒëŸ¬ëŒ€ëŠë¼ ì• ì“´ë‹¤ ê³µë§¤ìƒí™˜ í•˜ë ¤ë©´ ëª»ì˜¤ë¥´ê²Œ ëˆŒëŸ¬ì•¼ì§€ ê·¸ë ‡ì§€ë§Œ ê·¸ê²ƒë„ ê³§...   \n",
       "2                                 ì• ì“´ë‹¤ ë¹„ì‹¸ê²Œ.íŒ”ë ¤ê³  ì € ë§¤ìˆ˜í˜¸ê°€ ë´ë¼   \n",
       "3             ìœ ì•ˆíƒ€ .ì´ íƒ€ì´ë°ì— ë§¤ë„ë¥¼ í•˜ë‹ˆê¹Œ ë©”ì´ì € ì¦ê¶Œì‚¬ëŠ” ë‹¤ ëˆë²Œë•Œ ì ìì „í™˜í•˜ì§€   \n",
       "4                                    í”„ë¡œê·¸ë¨ ê°œ ê°„ë³´ë„¤ .ë†’ê²Œ íŒ”ë ¤ê³    \n",
       "...                                                 ...   \n",
       "6664                    ë¯¸êµ­í™•ì¥.. .êµ¿..ë°”ë¡œì´ê±°ì§€. ëª¨ë ˆì‹¤ì ë„ êµ¿. ë‹¬ë ¤ë³´ì   \n",
       "6665                                        ë‚´ì¼ .ì¥ ê´œì°®ì„ê¹Œìš”   \n",
       "6666                                          ì´ìš°í˜„.3ì¼ë‚¨ì•˜ë‹¤   \n",
       "6667        ê¹”ê¹” ë§ˆë…€ ë“±ì¥ .ì–´ì´êµ¬ í˜¸9ëœì•„ ì•„ì§ë„ ì–¸ë‹ˆë§ì•ˆë“£ê³  í˜„ì‹¤ë¶€ì •í•˜ëŠ” ëª»ë‚œì´ë“¤ ì—†   \n",
       "6668                         ì™¸ì¸ê¸°ê°„ë“¤ .ê¸‰ë“±í•¨ì„œ ìŒëŒì´ê°€ ëŒ€ì„¸ìƒìŠ¹ ëŒì…ì˜ˆìƒ   \n",
       "\n",
       "                                            contents_en  \\\n",
       "0     Please tell me which categories of the followi...   \n",
       "1     Let me know which categories of the following ...   \n",
       "2     Tell me which categories of the following sent...   \n",
       "3     Let me know which categories of the following ...   \n",
       "4     Tell me which categories of the following sent...   \n",
       "...                                                 ...   \n",
       "6664  Please tell me which categories of the followi...   \n",
       "6665  Tell me which categories of the following sent...   \n",
       "6666  Please tell me which category of the following...   \n",
       "6667  Let me know which categories of the following ...   \n",
       "6668  Let me know which categories of the following ...   \n",
       "\n",
       "                                                     qa  \n",
       "0     Earthquake in China's Xinjiang District 7.3. C...  \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "...                                                 ...  \n",
       "6664                                                     \n",
       "6665                              Tomorrow. Is it okay?  \n",
       "6666              Lee Woo -hyun. 3 There is a day left.  \n",
       "6667                                                     \n",
       "6668                                                     \n",
       "\n",
       "[6669 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d54bee56-dd1f-4d6b-a476-3d695f29c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f0753-1f50-40d1-9a82-6578b87a152d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39f5f919-1318-43ab-9cb1-a75fc5fd2046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d96b6c91f145089771fe68e714b194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/796k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e113d327c4d44df89ea91c69bae3838c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24c8f39ab56499396b8749d6862ba63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846d389b86ab480f82fbfbba2eb5b4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33fb7a67b34470bae1cb0287a339f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0084c18b419d484ba420856e9e4cf4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easynmt.EasyNMT:Exception: Helsinki-NLP/opus-mt-dv-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Helsinki-NLP/opus-mt-dv-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1722\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1722\u001b[0m     resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\utils\\hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\utils\\hub.py:495\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    494\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[1;32m--> 495\u001b[0m \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# The repo was not found and the user is not Authenticated\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m401 Client Error: Repository not found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     )\n\u001b[0;32m    422\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error: Repository not found for url: https://huggingface.co/Helsinki-NLP/opus-mt-dv-en/resolve/main/source.spm. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resen \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_contents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:154\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    153\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_doc \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    157\u001b[0m     output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:149\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m method_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [documents[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m    148\u001b[0m method_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_lang\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lng\n\u001b[1;32m--> 149\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, translated_sentences \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, translated):\n\u001b[0;32m    151\u001b[0m     output[idx] \u001b[38;5;241m=\u001b[39m translated_sentences\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:181\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     sent2doc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(splitted_sentences))\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m#logger.info(\"Sentence splitting done after: {:.2f} sec\".format(time.time() - start_time))\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m#logger.info(\"Translate {} sentences\".format(len(splitted_sentences)))\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m translated_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplitted_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Merge sentences back to documents\u001b[39;00m\n\u001b[0;32m    184\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:278\u001b[0m, in \u001b[0;36mEasyNMT.translate_sentences\u001b[1;34m(self, sentences, target_lang, source_lang, show_progress_bar, beam_size, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(iterator, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;241m/\u001b[39mscale, unit_scale\u001b[38;5;241m=\u001b[39mscale, smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m--> 278\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_sorted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m#Restore original sorting of sentences\u001b[39;00m\n\u001b[0;32m    281\u001b[0m output \u001b[38;5;241m=\u001b[39m [output[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\models\\OpusMT.py:40\u001b[0m, in \u001b[0;36mOpusMT.translate_sentences\u001b[1;34m(self, sentences, source_lang, target_lang, device, beam_size, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m], source_lang: \u001b[38;5;28mstr\u001b[39m, target_lang: \u001b[38;5;28mstr\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m, beam_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     39\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(source_lang, target_lang)\n\u001b[1;32m---> 40\u001b[0m     tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentences, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\models\\OpusMT.py:22\u001b[0m, in \u001b[0;36mOpusMT.load_model\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad model: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name)\n\u001b[1;32m---> 22\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1740\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1741\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1742\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1744\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1745\u001b[0m     )\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1751\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Helsinki-NLP/opus-mt-dv-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "resen = model.translate(all_contents,  target_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ad669-9275-4e29-91f8-d7824bf923c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3068c1c-3184-4e09-950b-a4b51e281c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ef59f-3f82-4317-a118-b61b443d664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bf573-fe14-4af4-9427-2b58d9d7398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01098344-2fa2-457b-8d14-20c73cdd7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Good night ğŸ˜Š\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp38]",
   "language": "python",
   "name": "conda-env-nlp38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
