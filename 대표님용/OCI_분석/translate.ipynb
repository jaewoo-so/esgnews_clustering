{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091d9cba-98e5-487e-aea3-86c92ff61baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import lib\n",
    "from textwrap import dedent\n",
    "from googletrans import Translator\n",
    "from tqdm.auto import tqdm\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77b6ed8-b903-482e-875b-0ba8900e0433",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('./scrape_oci.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8dd7077-91f7-4217-8fdc-778cca3aff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_korean(content):\n",
    "    content = content.replace('\\n\\n','')\n",
    "    content = ' '.join(content.split(' '))\n",
    "\n",
    "    #클리닝1\n",
    "    content = re.sub(r'... 기자', '', content)\n",
    "    content = re.sub(r'...기자', '', content)\n",
    "\n",
    "    #괄호안 제거 \n",
    "    ''' 괄호안 내용 제거하기 '''\n",
    "    pattern_bracket1 = r'\\([^)]*\\)'\n",
    "    pattern_bracket2 = r'\\{[^)]*\\}'\n",
    "    pattern_bracket3 = r'\\<[^)]*\\>'\n",
    "    pattern_bracket4 = r'\\[[^)]*\\]'\n",
    "\n",
    "    x = '이건 {괄호 안의 불필요한 정보를} 삭제하는 코드다.'\n",
    "    content = re.sub(pattern=pattern_bracket1, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    content = re.sub(pattern=pattern_bracket2, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    content = re.sub(pattern=pattern_bracket3, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    content = re.sub(pattern=pattern_bracket4, repl='', string= content)\n",
    "    content = ' '.join(content.split())\n",
    "    #클리닝\n",
    "    content = content.lower() #lower case\n",
    "    pattern = '([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)' # E-mail제거\n",
    "    content = re.sub(pattern=pattern, repl='', string=content)\n",
    "    pattern = '(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+' # URL제거\n",
    "    content = re.sub(pattern=pattern, repl='', string=content)\n",
    "    # 특수기호 특수\n",
    "    content = re.sub('[-=+#/\\?:^$@*\\\"※~%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`\\'…》Δ△·“=◎<>▷]', '', content)\n",
    "    content = re.sub('[一-龥]', '',content)\n",
    "    #pattern = '[^\\w\\s]'         # 특수기호제거\n",
    "    #content = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(content)) #remove punctuation\n",
    "    content = re.sub(r\"[^a-zA-Z0-9가-힣\\.\\s]\",\"\",content)\n",
    "    content = re.sub(pattern=pattern, repl='', string=content)\n",
    "    content = re.sub(r'\\s+', ' ', content) #remove extra space\n",
    "    content = re.sub(r'<[^>]+>','',content) #remove Html tags\n",
    "    content = re.sub(r'\\s+', ' ', content) #remove spaces\n",
    "    content = re.sub(r\"^\\s+\", '', content) #remove space from start\n",
    "    content = re.sub(r'\\s+$', '', content) #remove space from the end\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a18716b-d8d3-426f-a95e-1b985ad68b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contents = []\n",
    "for i, row in df.iterrows():\n",
    "    tit = row.Titles\n",
    "    cot = row.Contents\n",
    "    cot_all = str(tit) + '.' + str(cot)\n",
    "    all_contents.append(cleaning_korean(cot_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e69770b8-e0c8-42bb-8796-e0e924b9d878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd0e9d1d1c9469fbf216a3d40e1a1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6669 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 50.2 s\n",
      "Wall time: 2h 3min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "contents_en = []\n",
    "for x in tqdm(all_contents):\n",
    "    try:\n",
    "        src = \"다음의 문장이 긍정,중립,부정 중 어느 카테고리에 속하는지 알려줘 \" + '\"' + str(x) + '\"'\n",
    "        src_en = translator.translate(src).text\n",
    "        contents_en.append(src_en)\n",
    "    except:\n",
    "        contents_en.append('중립')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c95381-f083-44ec-a979-8875b9c6a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['contents_clean'] = all_contents\n",
    "df['contents_en'] = contents_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a24b72df-d159-481b-b53c-ca9ab73db394",
   "metadata": {},
   "outputs": [],
   "source": [
    "qq = []\n",
    "for en in contents_en: \n",
    "    res = re.findall(r'\\\".*?\\\"', en)\n",
    "    if res == []:\n",
    "        qq.append('')\n",
    "    else:\n",
    "        qq.append(res[0].replace('\"',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "552a6dd0-122f-47cc-9550-f1c3e2e0c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['qa'] = qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32f0055c-bacd-4500-8150-7c0f97dfd662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dates</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Contents</th>\n",
       "      <th>contents_clean</th>\n",
       "      <th>contents_en</th>\n",
       "      <th>qa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023.02.23 10:52</td>\n",
       "      <td>중국 신장지구에 지진 진도7.3</td>\n",
       "      <td>냉무</td>\n",
       "      <td>중국 신장지구에 지진 진도7.3.냉무</td>\n",
       "      <td>Please tell me which categories of the followi...</td>\n",
       "      <td>Earthquake in China's Xinjiang District 7.3. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023.02.23 10:51</td>\n",
       "      <td>신한 [2]</td>\n",
       "      <td>혼자 눌러대느라 애쓴다\\n공매상환 하려면 못오르게 눌러야지\\n그렇지만 그것도 곧한계...</td>\n",
       "      <td>신한 .혼자 눌러대느라 애쓴다 공매상환 하려면 못오르게 눌러야지 그렇지만 그것도 곧...</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023.02.23 10:50</td>\n",
       "      <td>애쓴다 비싸게</td>\n",
       "      <td>팔려고 저 매수호가 봐라 ㅋㅋ</td>\n",
       "      <td>애쓴다 비싸게.팔려고 저 매수호가 봐라</td>\n",
       "      <td>Tell me which categories of the following sent...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023.02.23 10:42</td>\n",
       "      <td>유안타 ㅉㅉ</td>\n",
       "      <td>이 타이밍에 매도를 하니까 메이저 증권사는 다 돈벌때 적자전환하지 ㅋㅋㅋ</td>\n",
       "      <td>유안타 .이 타이밍에 매도를 하니까 메이저 증권사는 다 돈벌때 적자전환하지</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023.02.23 10:33</td>\n",
       "      <td>프로그램 개 간보네 [1]</td>\n",
       "      <td>높게 팔려고 ㅋㅋㅋ</td>\n",
       "      <td>프로그램 개 간보네 .높게 팔려고</td>\n",
       "      <td>Tell me which categories of the following sent...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6664</th>\n",
       "      <td>2022.10.24 09:07</td>\n",
       "      <td>미국확장.. [2]</td>\n",
       "      <td>굿..바로이거지.\\n모레실적도 굿.\\n달려보자</td>\n",
       "      <td>미국확장.. .굿..바로이거지. 모레실적도 굿. 달려보자</td>\n",
       "      <td>Please tell me which categories of the followi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6665</th>\n",
       "      <td>2022.10.23 23:10</td>\n",
       "      <td>내일 [2]</td>\n",
       "      <td>장 괜찮을까요</td>\n",
       "      <td>내일 .장 괜찮을까요</td>\n",
       "      <td>Tell me which categories of the following sent...</td>\n",
       "      <td>Tomorrow. Is it okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>2022.10.23 18:51</td>\n",
       "      <td>이우현</td>\n",
       "      <td>3일남았다</td>\n",
       "      <td>이우현.3일남았다</td>\n",
       "      <td>Please tell me which category of the following...</td>\n",
       "      <td>Lee Woo -hyun. 3 There is a day left.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>2022.10.23 15:03</td>\n",
       "      <td>깔깔 마녀 등장!!!!!!!! [6]</td>\n",
       "      <td>어이구 호9덜아 아직도 언니말안듣고 현실부정하는 못난이들 없ㅈ ㅔ~? ㅋ ㅔ~</td>\n",
       "      <td>깔깔 마녀 등장 .어이구 호9덜아 아직도 언니말안듣고 현실부정하는 못난이들 없</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668</th>\n",
       "      <td>2022.10.23 12:43</td>\n",
       "      <td>외인기간들 [1]</td>\n",
       "      <td>급등함서 쌍끌이가\\n대세상승 돌입예상</td>\n",
       "      <td>외인기간들 .급등함서 쌍끌이가 대세상승 돌입예상</td>\n",
       "      <td>Let me know which categories of the following ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6669 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dates                Titles  \\\n",
       "0     2023.02.23 10:52     중국 신장지구에 지진 진도7.3   \n",
       "1     2023.02.23 10:51                신한 [2]   \n",
       "2     2023.02.23 10:50               애쓴다 비싸게   \n",
       "3     2023.02.23 10:42                유안타 ㅉㅉ   \n",
       "4     2023.02.23 10:33        프로그램 개 간보네 [1]   \n",
       "...                ...                   ...   \n",
       "6664  2022.10.24 09:07            미국확장.. [2]   \n",
       "6665  2022.10.23 23:10                내일 [2]   \n",
       "6666  2022.10.23 18:51                   이우현   \n",
       "6667  2022.10.23 15:03  깔깔 마녀 등장!!!!!!!! [6]   \n",
       "6668  2022.10.23 12:43             외인기간들 [1]   \n",
       "\n",
       "                                               Contents  \\\n",
       "0                                                    냉무   \n",
       "1     혼자 눌러대느라 애쓴다\\n공매상환 하려면 못오르게 눌러야지\\n그렇지만 그것도 곧한계...   \n",
       "2                                      팔려고 저 매수호가 봐라 ㅋㅋ   \n",
       "3              이 타이밍에 매도를 하니까 메이저 증권사는 다 돈벌때 적자전환하지 ㅋㅋㅋ   \n",
       "4                                            높게 팔려고 ㅋㅋㅋ   \n",
       "...                                                 ...   \n",
       "6664                          굿..바로이거지.\\n모레실적도 굿.\\n달려보자   \n",
       "6665                                            장 괜찮을까요   \n",
       "6666                                              3일남았다   \n",
       "6667        어이구 호9덜아 아직도 언니말안듣고 현실부정하는 못난이들 없ㅈ ㅔ~? ㅋ ㅔ~   \n",
       "6668                               급등함서 쌍끌이가\\n대세상승 돌입예상   \n",
       "\n",
       "                                         contents_clean  \\\n",
       "0                                  중국 신장지구에 지진 진도7.3.냉무   \n",
       "1     신한 .혼자 눌러대느라 애쓴다 공매상환 하려면 못오르게 눌러야지 그렇지만 그것도 곧...   \n",
       "2                                 애쓴다 비싸게.팔려고 저 매수호가 봐라   \n",
       "3             유안타 .이 타이밍에 매도를 하니까 메이저 증권사는 다 돈벌때 적자전환하지   \n",
       "4                                    프로그램 개 간보네 .높게 팔려고   \n",
       "...                                                 ...   \n",
       "6664                    미국확장.. .굿..바로이거지. 모레실적도 굿. 달려보자   \n",
       "6665                                        내일 .장 괜찮을까요   \n",
       "6666                                          이우현.3일남았다   \n",
       "6667        깔깔 마녀 등장 .어이구 호9덜아 아직도 언니말안듣고 현실부정하는 못난이들 없   \n",
       "6668                         외인기간들 .급등함서 쌍끌이가 대세상승 돌입예상   \n",
       "\n",
       "                                            contents_en  \\\n",
       "0     Please tell me which categories of the followi...   \n",
       "1     Let me know which categories of the following ...   \n",
       "2     Tell me which categories of the following sent...   \n",
       "3     Let me know which categories of the following ...   \n",
       "4     Tell me which categories of the following sent...   \n",
       "...                                                 ...   \n",
       "6664  Please tell me which categories of the followi...   \n",
       "6665  Tell me which categories of the following sent...   \n",
       "6666  Please tell me which category of the following...   \n",
       "6667  Let me know which categories of the following ...   \n",
       "6668  Let me know which categories of the following ...   \n",
       "\n",
       "                                                     qa  \n",
       "0     Earthquake in China's Xinjiang District 7.3. C...  \n",
       "1                                                        \n",
       "2                                                        \n",
       "3                                                        \n",
       "4                                                        \n",
       "...                                                 ...  \n",
       "6664                                                     \n",
       "6665                              Tomorrow. Is it okay?  \n",
       "6666              Lee Woo -hyun. 3 There is a day left.  \n",
       "6667                                                     \n",
       "6668                                                     \n",
       "\n",
       "[6669 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d54bee56-dd1f-4d6b-a476-3d695f29c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easynmt import EasyNMT\n",
    "model = EasyNMT('opus-mt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f0753-1f50-40d1-9a82-6578b87a152d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39f5f919-1318-43ab-9cb1-a75fc5fd2046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d96b6c91f145089771fe68e714b194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/796k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e113d327c4d44df89ea91c69bae3838c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24c8f39ab56499396b8749d6862ba63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.23M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846d389b86ab480f82fbfbba2eb5b4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a33fb7a67b34470bae1cb0287a339f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0084c18b419d484ba420856e9e4cf4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/281M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:easynmt.EasyNMT:Exception: Helsinki-NLP/opus-mt-dv-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Helsinki-NLP/opus-mt-dv-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1722\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1722\u001b[0m     resolved_vocab_files[file_id] \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\utils\\hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\utils\\hub.py:495\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    494\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[1;32m--> 495\u001b[0m \u001b[43m_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    496\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36m_raise_for_status\u001b[1;34m(response)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# The repo was not found and the user is not Authenticated\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m401 Client Error: Repository not found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the repo is private, make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     )\n\u001b[0;32m    422\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error: Repository not found for url: https://huggingface.co/Helsinki-NLP/opus-mt-dv-en/resolve/main/source.spm. If the repo is private, make sure you are authenticated.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resen \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_contents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:154\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    153\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_single_doc \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    157\u001b[0m     output \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:149\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m method_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocuments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [documents[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m ids]\n\u001b[0;32m    148\u001b[0m method_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_lang\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m lng\n\u001b[1;32m--> 149\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmethod_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, translated_sentences \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(ids, translated):\n\u001b[0;32m    151\u001b[0m     output[idx] \u001b[38;5;241m=\u001b[39m translated_sentences\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:181\u001b[0m, in \u001b[0;36mEasyNMT.translate\u001b[1;34m(self, documents, target_lang, source_lang, show_progress_bar, beam_size, batch_size, perform_sentence_splitting, paragraph_split, sentence_splitter, document_language_detection, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m     sent2doc\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlen\u001b[39m(splitted_sentences))\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m#logger.info(\"Sentence splitting done after: {:.2f} sec\".format(time.time() - start_time))\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m#logger.info(\"Translate {} sentences\".format(len(splitted_sentences)))\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m translated_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplitted_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Merge sentences back to documents\u001b[39;00m\n\u001b[0;32m    184\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\EasyNMT.py:278\u001b[0m, in \u001b[0;36mEasyNMT.translate_sentences\u001b[1;34m(self, sentences, target_lang, source_lang, show_progress_bar, beam_size, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(iterator, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sentences)\u001b[38;5;241m/\u001b[39mscale, unit_scale\u001b[38;5;241m=\u001b[39mscale, smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start_idx \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m--> 278\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_sorted\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_lang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m#Restore original sorting of sentences\u001b[39;00m\n\u001b[0;32m    281\u001b[0m output \u001b[38;5;241m=\u001b[39m [output[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\models\\OpusMT.py:40\u001b[0m, in \u001b[0;36mOpusMT.translate_sentences\u001b[1;34m(self, sentences, source_lang, target_lang, device, beam_size, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtranslate_sentences\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentences: List[\u001b[38;5;28mstr\u001b[39m], source_lang: \u001b[38;5;28mstr\u001b[39m, target_lang: \u001b[38;5;28mstr\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m, beam_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     39\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHelsinki-NLP/opus-mt-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(source_lang, target_lang)\n\u001b[1;32m---> 40\u001b[0m     tokenizer, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     43\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(sentences, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\easynmt\\models\\OpusMT.py:22\u001b[0m, in \u001b[0;36mOpusMT.load_model\u001b[1;34m(self, model_name)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad model: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mmodel_name)\n\u001b[1;32m---> 22\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMarianTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m MarianMTModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m     24\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mD:\\anaconda3\\envs\\nlp38\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1740\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1741\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1742\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1744\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1745\u001b[0m     )\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1751\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Helsinki-NLP/opus-mt-dv-en is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "resen = model.translate(all_contents,  target_lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ad669-9275-4e29-91f8-d7824bf923c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3068c1c-3184-4e09-950b-a4b51e281c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429ef59f-3f82-4317-a118-b61b443d664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bf573-fe14-4af4-9427-2b58d9d7398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "model.save_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01098344-2fa2-457b-8d14-20c73cdd7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Good night 😊\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp38]",
   "language": "python",
   "name": "conda-env-nlp38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
